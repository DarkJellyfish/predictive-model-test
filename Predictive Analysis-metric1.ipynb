{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('predictive_maintenance.csv')\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_df = df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list numerical cols that are not necessarily continuous\n",
    "numerical_cols = [col for col in df.columns if \n",
    "                 (df[col].dtype=='int64' or df[col].dtype=='float64')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_cols].describe().reindex(['min', 'max', 'mean', '50%'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns with data where:\n",
    "# the mean and 50th percentile are significantly different where many points are close to 0\n",
    "# there is not useful data\n",
    "cols_to_drop = ['date', 'device', 'metric2', 'metric3', 'metric4', 'metric7', 'metric8', 'metric9']\n",
    "df = df.drop(cols_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the distribution of values in metrics 1, 5 and 6\n",
    "# info in metric1 is not very varied\n",
    "plt.hist(df['metric1'], bins=30)\n",
    "plt.xlabel('metric1')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['metric5'], bins=30)\n",
    "plt.xlabel('metric5')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df['metric6'], bins=30)\n",
    "plt.xlabel('metric6')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirms metric1 is not a good sources of info\n",
    "plt.scatter(x=range(len(list(df['metric1'][df['failure']==1]))),\n",
    "           y=df['metric1'][df['failure']==1], s=1)\n",
    "plt.xlabel('Successes (rows)')\n",
    "plt.ylabel('metric1')\n",
    "plt.title('Metric1 vlaues with failure (failure = 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most failures happend with metric 5 values between 0 and 20\n",
    "plt.scatter(x=range(len(list(df['metric5'][df['failure']==1]))),\n",
    "           y=df['metric5'][df['failure']==1], s=1)\n",
    "plt.xlabel('Successes (rows)')\n",
    "plt.ylabel('metric5')\n",
    "plt.title('Metric5 vlaues with failure (failure = 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most failures happend with metric 5 values between 200000 and 350000\n",
    "plt.scatter(x=range(len(list(df['metric6'][df['failure']==1]))),\n",
    "           y=df['metric6'][df['failure']==1], s=1)\n",
    "plt.xlabel('Successes (rows)')\n",
    "plt.ylabel('metric6')\n",
    "plt.title('Metric6 vlaues with failure (failure = 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups metric1 values into buckets by % of failure\n",
    "metric1_bucket = df.groupby(pd.cut(df['metric1'], bins=\n",
    "                                  [0, .5, 1, 1.5, 2, 2.5]))\n",
    "metric1_bucket = round((metric1_bucket.sum()['failure']/metric1_bucket.size())*100, 2)\n",
    "metric1_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups metric5 values into buckets by % of failure\n",
    "metric5_bucket = df.groupby(pd.cut(df['metric5'], bins=\n",
    "                                  [x for x in range(0, 110, 10)]))\n",
    "metric5_bucket = round((metric5_bucket.sum()['failure']/metric5_bucket.size())*100, 2)\n",
    "metric5_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groups metric6 values into buckets by % of failure\n",
    "metric6_bucket = df.groupby(pd.cut(df['metric6'], bins=\n",
    "                                   [x for x in range(0, 770000, 70000)]))\n",
    "metric6_bucket = round((metric6_bucket.sum()['failure']/metric6_bucket.size())*100, 2)\n",
    "metric6_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the buckets\n",
    "x = [str(i)+'-'+str(i+10) for i in range(0, 100, 10)]\n",
    "plt.plot(x, metric5_bucket.values)\n",
    "plt.xlabel('metric5 group')\n",
    "plt.ylabel('% failure')\n",
    "plt.title('% of devices in metric5 group that failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the buckets\n",
    "x = [str(i)+'-'+str(i+7) for i in range(0, 70, 7)]\n",
    "plt.plot(x, metric6_bucket.values)\n",
    "plt.xlabel('metric6 group (in 00000s)')\n",
    "plt.ylabel('% failure')\n",
    "plt.title('% of devices in metric6 group that failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing metric5 and metric6 colums with their respective buckets\n",
    "df['metric5'] = pd.cut(df['metric5'], bins=\n",
    "                       [x for x in range(0, 110, 10)])\n",
    "df['metric6'] = pd.cut(df['metric6'], bins=[x for x in range(0, 770000, 70000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping 2 dummy variable columns as they are implied\n",
    "cols_to_drop3 = ['metric5_(0, 10]', 'metric6_(0, 70000]']\n",
    "df = df.drop(cols_to_drop3, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving a clean copy\n",
    "df.to_csv('Clean_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = df.drop(columns=['failure'], axis=1)\n",
    "label = df['failure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(feat, label, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_x = StandardScaler()\n",
    "X_train = sc_x.fit_transform(X_train)\n",
    "X_test = sc_x.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_vector_classifier = SVC(kernel='rbf')\n",
    "support_vector_classifier.fit(X_train, y_train)\n",
    "y_pred_svc = support_vector_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_support_vector_classifier = confusion_matrix(y_test, y_pred_svc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cm_support_vector_classifier, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = cm_support_vector_classifier[0][0] + cm_support_vector_classifier[1][1]\n",
    "den = sum(cm_support_vector_classifier[0]) + sum(cm_support_vector_classifier[1])\n",
    "acc_svc = (num/den)*100\n",
    "print('Accuracy: ', round(acc_svc, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using cross validation, rechecks the data model\n",
    "cross_val_svc = cross_val_score(estimator=\n",
    "                                SVC(kernel='rbf'),\n",
    "                                   X=X_train, y=y_train, cv=10,\n",
    "                                   n_jobs=-1)\n",
    "\n",
    "print('Cross Validation Accruacy:', round(cross_val_svc.mean()\n",
    "                                         *100, 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
